{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811877b8",
   "metadata": {},
   "source": [
    "# 1 Newton-Raphson算法实现逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b962f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef3300b",
   "metadata": {},
   "source": [
    "具体来说，给定一个训练集 $D={(\\mathbf{x}_1,y_1),\\ldots,(\\mathbf{x}_n,y_n)}$，其中 $\\mathbf{x}_i\\in \\mathbb{R}^d$ 是一个d维特征向量，$y_i\\in {0,1}$ 是对应的标签。逻辑回归的模型参数是一个d维向量 $\\mathbf{w}$。逻辑回归模型对于给定的特征向量 $\\mathbf{x}$，输出一个标量值 $\\hat{y}=\\sigma(\\mathbf{w}^T\\mathbf{x})$，其中 $\\sigma(\\cdot)$ 是sigmoid函数，可以将实数映射到$(0,1)$区间。\n",
    "\n",
    "假设我们使用对数似然函数作为损失函数，则我们的优化目标是：\n",
    "\n",
    "$$\\min_{\\mathbf{w}} L(\\mathbf{w}) = -\\sum_{i=1}^n [y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$$\n",
    "\n",
    "通过求解 $L(\\mathbf{w})$ 的一阶和二阶导数，可以得到如下牛顿法公式：\n",
    "\n",
    "$$\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - [\\nabla^2 L(\\mathbf{w}^{(t)})]^{-1} \\nabla L(\\mathbf{w}^{(t)})$$\n",
    "\n",
    "其中 $\\nabla L(\\mathbf{w})$ 和 $\\nabla^2 L(\\mathbf{w})$ 分别是对数似然函数 $L(\\mathbf{w})$ 的一阶和二阶导数：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def29378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(beta, N, seed):\n",
    "    np.random.seed(seed)\n",
    "    X = np.concatenate((np.ones((N, 1)), np.random.normal(size=(N, 3))), axis=1)\n",
    "    y_prob = 1 / (1 + np.exp(- X @ beta))  # 标签概率值\n",
    "    y = np.random.binomial(1, y_prob)  # 真实标签，由标签概率值和二项分布得到\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66c7626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"Logistic Regression Classifier training by Newton Method.\"\"\"\n",
    "    def __init__(self, error, max_epoch):\n",
    "        self.error = error\n",
    "        self.max_epoch = max_epoch\n",
    "        self.beta = np.zeros((4, 1))\n",
    "    \n",
    "    def newton_method(self, X, y):\n",
    "        for _ in range(self.max_epoch):\n",
    "            p = 1 / (1 + np.exp(- X @ self.beta))\n",
    "            print(f'loop begin---\\nbeta.shape {self.beta.shape} X.shape{X.shape} p.shape {p.shape}')\n",
    "            W = np.diag((p * (1 - p)).flatten())\n",
    "            print(f'W.shape {W.shape}')\n",
    "            print(f'p.shape{p.shape} y.shape {y.shape}')\n",
    "            diff = X.T @ (p - y.reshape(-1, 1))\n",
    "            print(f'X.T.shape{X.T.shape} ((p - y)).shape{((p - y)).shape}')\n",
    "#             print(np.matmul(X.T, W).shape)\n",
    "            hess = X.T @ W @ X\n",
    "            beta_new = self.beta - np.linalg.inv(hess) @ diff\n",
    "            print(f'self.beta.shape {self.beta.shape} diff.shape {diff.shape} hess.shape {hess.shape} beta_new.shape {beta_new.shape}')\n",
    "            \n",
    "            if np.linalg.norm(beta_new - self.beta, np.inf) <= self.error:\n",
    "                break\n",
    "            else:\n",
    "                self.beta = beta_new\n",
    "            print(f'loop done-----\\n')\n",
    "        return self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37b4a90c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=200, r=1\n",
      "X.shape (200, 4)\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(200, 4) p.shape (200, 1)\n",
      "W.shape (200, 200)\n",
      "p.shape(200, 1) y.shape (200,)\n",
      "X.T.shape(4, 200) ((p - y)).shape(200, 200)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(200, 4) p.shape (200, 1)\n",
      "W.shape (200, 200)\n",
      "p.shape(200, 1) y.shape (200,)\n",
      "X.T.shape(4, 200) ((p - y)).shape(200, 200)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(200, 4) p.shape (200, 1)\n",
      "W.shape (200, 200)\n",
      "p.shape(200, 1) y.shape (200,)\n",
      "X.T.shape(4, 200) ((p - y)).shape(200, 200)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(200, 4) p.shape (200, 1)\n",
      "W.shape (200, 200)\n",
      "p.shape(200, 1) y.shape (200,)\n",
      "X.T.shape(4, 200) ((p - y)).shape(200, 200)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(200, 4) p.shape (200, 1)\n",
      "W.shape (200, 200)\n",
      "p.shape(200, 1) y.shape (200,)\n",
      "X.T.shape(4, 200) ((p - y)).shape(200, 200)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(200, 4) p.shape (200, 1)\n",
      "W.shape (200, 200)\n",
      "p.shape(200, 1) y.shape (200,)\n",
      "X.T.shape(4, 200) ((p - y)).shape(200, 200)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "beta_newton [[-0.53722067]\n",
      " [ 0.71112495]\n",
      " [-1.10887444]\n",
      " [ 1.2042508 ]]\n",
      "N=500, r=2\n",
      "X.shape (500, 4)\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(500, 4) p.shape (500, 1)\n",
      "W.shape (500, 500)\n",
      "p.shape(500, 1) y.shape (500,)\n",
      "X.T.shape(4, 500) ((p - y)).shape(500, 500)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(500, 4) p.shape (500, 1)\n",
      "W.shape (500, 500)\n",
      "p.shape(500, 1) y.shape (500,)\n",
      "X.T.shape(4, 500) ((p - y)).shape(500, 500)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(500, 4) p.shape (500, 1)\n",
      "W.shape (500, 500)\n",
      "p.shape(500, 1) y.shape (500,)\n",
      "X.T.shape(4, 500) ((p - y)).shape(500, 500)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(500, 4) p.shape (500, 1)\n",
      "W.shape (500, 500)\n",
      "p.shape(500, 1) y.shape (500,)\n",
      "X.T.shape(4, 500) ((p - y)).shape(500, 500)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(500, 4) p.shape (500, 1)\n",
      "W.shape (500, 500)\n",
      "p.shape(500, 1) y.shape (500,)\n",
      "X.T.shape(4, 500) ((p - y)).shape(500, 500)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(500, 4) p.shape (500, 1)\n",
      "W.shape (500, 500)\n",
      "p.shape(500, 1) y.shape (500,)\n",
      "X.T.shape(4, 500) ((p - y)).shape(500, 500)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "beta_newton [[-0.52080574]\n",
      " [ 0.33426287]\n",
      " [-1.29667795]\n",
      " [ 0.99257345]]\n",
      "N=750, r=3\n",
      "X.shape (750, 4)\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(750, 4) p.shape (750, 1)\n",
      "W.shape (750, 750)\n",
      "p.shape(750, 1) y.shape (750,)\n",
      "X.T.shape(4, 750) ((p - y)).shape(750, 750)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(750, 4) p.shape (750, 1)\n",
      "W.shape (750, 750)\n",
      "p.shape(750, 1) y.shape (750,)\n",
      "X.T.shape(4, 750) ((p - y)).shape(750, 750)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(750, 4) p.shape (750, 1)\n",
      "W.shape (750, 750)\n",
      "p.shape(750, 1) y.shape (750,)\n",
      "X.T.shape(4, 750) ((p - y)).shape(750, 750)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(750, 4) p.shape (750, 1)\n",
      "W.shape (750, 750)\n",
      "p.shape(750, 1) y.shape (750,)\n",
      "X.T.shape(4, 750) ((p - y)).shape(750, 750)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(750, 4) p.shape (750, 1)\n",
      "W.shape (750, 750)\n",
      "p.shape(750, 1) y.shape (750,)\n",
      "X.T.shape(4, 750) ((p - y)).shape(750, 750)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "beta_newton [[-0.42824642]\n",
      " [ 0.47715437]\n",
      " [-0.90093826]\n",
      " [ 0.78644565]]\n",
      "N=1000, r=4\n",
      "X.shape (1000, 4)\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(1000, 4) p.shape (1000, 1)\n",
      "W.shape (1000, 1000)\n",
      "p.shape(1000, 1) y.shape (1000,)\n",
      "X.T.shape(4, 1000) ((p - y)).shape(1000, 1000)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(1000, 4) p.shape (1000, 1)\n",
      "W.shape (1000, 1000)\n",
      "p.shape(1000, 1) y.shape (1000,)\n",
      "X.T.shape(4, 1000) ((p - y)).shape(1000, 1000)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(1000, 4) p.shape (1000, 1)\n",
      "W.shape (1000, 1000)\n",
      "p.shape(1000, 1) y.shape (1000,)\n",
      "X.T.shape(4, 1000) ((p - y)).shape(1000, 1000)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(1000, 4) p.shape (1000, 1)\n",
      "W.shape (1000, 1000)\n",
      "p.shape(1000, 1) y.shape (1000,)\n",
      "X.T.shape(4, 1000) ((p - y)).shape(1000, 1000)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "loop done-----\n",
      "\n",
      "loop begin---\n",
      "beta.shape (4, 1) X.shape(1000, 4) p.shape (1000, 1)\n",
      "W.shape (1000, 1000)\n",
      "p.shape(1000, 1) y.shape (1000,)\n",
      "X.T.shape(4, 1000) ((p - y)).shape(1000, 1000)\n",
      "self.beta.shape (4, 1) diff.shape (4, 1) hess.shape (4, 4) beta_new.shape (4, 1)\n",
      "beta_newton [[-0.47807779]\n",
      " [ 0.4997258 ]\n",
      " [-1.05123075]\n",
      " [ 1.15510262]]\n"
     ]
    }
   ],
   "source": [
    "beta = np.array([-0.5, 0.5, -1, 1]).T\n",
    "Ns = [200, 500, 750, 1000]\n",
    "\n",
    "for i, N in enumerate(Ns):\n",
    "    print(f\"N={N}, r={i + 1}\")\n",
    "    X, y = generate_data(beta, N, i + 1)\n",
    "    print(f'X.shape {X.shape}')\n",
    "    lr = LogisticRegression(error=1e-5, max_epoch=200)\n",
    "    beta_newton = lr.newton_method(X, y)\n",
    "    print(f'beta_newton {beta_newton}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1428d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \"\"\"Logistic Regression Classifier training by Newton Method.\"\"\"\n",
    "    def __init__(self, error: float = 1e-5, max_epoch: int = 200):\n",
    "        \"\"\"\n",
    "        :param error: float, if the distance between new weight and \n",
    "                      old weight is less than error, the process \n",
    "                      of traing will break.\n",
    "        :param max_epoch: if training epoch >= max_epoch the process \n",
    "                          of traing will break.\n",
    "        \"\"\"\n",
    "        self.error = error\n",
    "        self.max_epoch = max_epoch\n",
    "        self.weight = None\n",
    "        self.sign = np.vectorize(lambda x: 1 if x >= 0.5 else 0)\n",
    "\n",
    "\n",
    "    def diff(self, X_, y, p):\n",
    "        \"\"\"Get derivative\n",
    "        :param X_: shape = (n_samples, n_features + 1) \n",
    "        :param y: shape = (n_samples)\n",
    "        :param p: shape = (n_samples) P(y=1 | x)\n",
    "        :return:  shape = (n_features + 1) first derivative\n",
    "        \"\"\"\n",
    "        return -(y - p) @ X_\n",
    "\n",
    "\n",
    "    def newton_method(self, X_, y):\n",
    "        \"\"\"Newton Method to calculate weight\n",
    "        :param X_: shape = (n_samples + 1, n_features)\n",
    "        :param y: shape = (n_samples)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.weight = np.ones(X_.shape[1])\n",
    "        self.X_XT = []\n",
    "        for i in range(X_.shape[0]):\n",
    "            t = X_[i, :].reshape((-1, 1))\n",
    "            self.X_XT.append(t @ t.T)\n",
    "\n",
    "        for _ in range(self.max_epoch):\n",
    "            p = self.p_func(X_)\n",
    "            diff = self.diff(X_, y, p)\n",
    "            hess = self.hess_mat(X_, p)\n",
    "            new_weight = self.weight - (np.linalg.inv(hess) @ diff.reshape((-1, 1))).flatten()\n",
    "\n",
    "            if np.linalg.norm(new_weight - self.weight) <= self.error:\n",
    "                break\n",
    "            self.weight = new_weight\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X_: shape = (n_samples, n_features)\n",
    "        :param y: shape = (n_samples)\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        X_ = np.c_[np.ones(X.shape[0]), X]\n",
    "        self.newton_method(X_, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X) -> np.array:\n",
    "        \"\"\"\n",
    "        :param X: shape = (n_samples, n_features] \n",
    "        :return: shape = (n_samples]\n",
    "        \"\"\"\n",
    "        X_ = np.c_[np.ones(X.shape[0]), X]\n",
    "        return self.sign(self.p_func(X_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24562532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sf/xs8l953s5q7f77gl88nwhn8r0000gn/T/ipykernel_72071/3399066531.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     lr = LogisticRegression(error=1e-5, max_epoch=200)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     lr \u001b[38;5;241m=\u001b[39m LogisticRegression(tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimated Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lr\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, beta_true)\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"Fit logistic regression model to data.\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mc_[np\u001b[38;5;241m.\u001b[39mones(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), X]  \u001b[38;5;66;03m# add intercept term\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewton_raphson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mLogisticRegression.newton_raphson\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     29\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient(X, y, w)\n\u001b[1;32m     30\u001b[0m hess \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhessian(X, w)\n\u001b[0;32m---> 31\u001b[0m w_new \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m-\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhess\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m@\u001b[39m grad\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(w_new \u001b[38;5;241m-\u001b[39m w) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/numpy/linalg/linalg.py:545\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    543\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    544\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 545\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/numpy/linalg/linalg.py:88\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "beta_true = np.array([-0.5, 0.5, -1, 1])\n",
    "Ns = [200, 500, 750, 1000]\n",
    "\n",
    "for i, N in enumerate(Ns):\n",
    "    print(f\"N={N}\")\n",
    "    X, y = generate_data(beta_true, N, i + 1)\n",
    "    \n",
    "#     lr = LogisticRegression(error=1e-5, max_epoch=200)\n",
    "    lr = LogisticRegression(tol=1e-5, max_iter=200)\n",
    "    lr.fit(X, y)\n",
    "    print(\"Estimated Parameters:\", lr.weight)\n",
    "    print(\"True Parameters:\", beta_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f032d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    \"\"\"Logistic Regression Classifier training by Newton Method.\"\"\"\n",
    "    def __init__(self, tol: float = 1e-5, max_iter: int = 200):\n",
    "        \"\"\"\n",
    "        :param tol: tolerance for stopping criterion.\n",
    "        :param max_iter: maximum number of iterations.\n",
    "        \"\"\"\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def hessian(self, X, w):\n",
    "        \"\"\"Hessian matrix.\"\"\"\n",
    "        p = self.sigmoid(X @ w)\n",
    "        return (X.T * p * (1 - p)) @ X\n",
    "    \n",
    "    def gradient(self, X, y, w):\n",
    "        \"\"\"Gradient of negative log likelihood.\"\"\"\n",
    "        p = self.sigmoid(X @ w)\n",
    "        return X.T @ (p - y)\n",
    "    \n",
    "    def newton_raphson(self, X, y):\n",
    "        \"\"\"Newton-Raphson algorithm.\"\"\"\n",
    "        w = np.zeros(X.shape[1])\n",
    "        for _ in range(self.max_iter):\n",
    "            grad = self.gradient(X, y, w)\n",
    "            hess = self.hessian(X, w)\n",
    "            w_new = w - np.linalg.inv(hess) @ grad\n",
    "            if np.linalg.norm(w_new - w) < self.tol:\n",
    "                break\n",
    "            w = w_new\n",
    "        self.coef_ = w\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit logistic regression model to data.\"\"\"\n",
    "        X = np.c_[np.ones(X.shape[0]), X]  # add intercept term\n",
    "        self.newton_raphson(X, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        X = np.c_[np.ones(X.shape[0]), X]  # add intercept term\n",
    "        return self.sigmoid(X @ self.coef_)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
